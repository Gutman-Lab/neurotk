{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Datasets\n",
    "Create some test datasets on DSA for developing NeuroTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from dsa_helpers.girder_utils import login, get_items\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to reconstruct the nested dictionary (source: ChatGPT)\n",
    "def reconstruct_nested(flattened, sep):\n",
    "    nested = {}\n",
    "    for key, value in flattened.items():\n",
    "        keys = key.split(sep)\n",
    "        d = nested\n",
    "        for part in keys[:-1]:\n",
    "            if part not in d:\n",
    "                d[part] = {}\n",
    "            d = d[part]\n",
    "        d[keys[-1]] = value\n",
    "    return nested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate girder client.\n",
    "gc = login(\"http://bdsa.pathology.emory.edu:8080/api/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter items to only those verified, not control, and not bad images.\n",
    "# Secondary, filter to only Tau stains.\n",
    "items_by_case = {}  # save the items grouped by their case ID.\n",
    "items = []\n",
    "\n",
    "# Iterate through the year folders in the Emory ADRC collection.\n",
    "for fld in gc.listFolder(\"673133b4900c0c05599779aa\"):\n",
    "    # Only check 2020, 2022, 2023 and 2024, which I have verified.\n",
    "    if fld['name'] in ('2020', '2022', '2023', '2024'):\n",
    "        # Check items in the folder.\n",
    "        for item in get_items(gc, fld['_id']):\n",
    "            if 'meta' not in item:\n",
    "                meta = {}\n",
    "            else:\n",
    "                meta = item['meta']\n",
    "                \n",
    "            if 'npSchema' not in meta:\n",
    "                continue\n",
    "                \n",
    "            # Only include good images that are verified and are not control.\n",
    "            if (\n",
    "                meta.get('control') != \"yes\" and \\\n",
    "                meta.get(\"bad_image\") == \"no\" and \\\n",
    "                meta['npSchema'].get('verified') == \"yes\" and \\\n",
    "                meta['npSchema'].get('stainID') == \"Tau\"\n",
    "            ):\n",
    "                case_id = meta['npSchema']['caseID']\n",
    "                \n",
    "                if case_id not in items_by_case:\n",
    "                    items_by_case[case_id] = []\n",
    "                    \n",
    "                items_by_case[case_id].append(item)\n",
    "                items.append(item)\n",
    "                \n",
    "print(f\"Total number of cases: {len(items_by_case)}\")\n",
    "print(f\"Total number of images: {len(items)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(items, sep=\":\").fillna(\"\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the unique region names.\n",
    "for k, v in df['meta:npSchema:regionName'].value_counts().items():\n",
    "    if k != \"\":\n",
    "        print(f\"{k} (n={v})\")\n",
    "        \n",
    "# Specify the regions that I want to include.\n",
    "regions = {\n",
    "    \"Amygdala\": \"Amygdala\",\n",
    "    \"Temporal cortex\": \"Temporal cortex\",\n",
    "    \"Hippocampus\": \"Hippocampus\",\n",
    "    \"Occipital cortex\": \"Occipital cortex\",\n",
    "    \"Left hippocampus\": \"Hippocampus\",\n",
    "    \"Right hippocampus\": \"Hippocampus\",\n",
    "}\n",
    "\n",
    "# Get a count of the unique region values.\n",
    "n_regions = len(set(regions.values()))\n",
    "print(f\"Total number of regions: {n_regions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Loop through the cases.\n",
    "Only include cases that have images from all the selected regions.\n",
    "Only include on image from each region.\n",
    "\"\"\"\n",
    "neurotk_items = []\n",
    "neurotk_items_small = []\n",
    "case_count = 0\n",
    "\n",
    "for case_id, case_items in items_by_case.items():\n",
    "    # Convert the case items to a dataframe.\n",
    "    case_df = pd.json_normalize(case_items, sep=\":\").fillna(\"\")\n",
    "    \n",
    "    # Add a region name column, whose value is set by the regions dictionary.\n",
    "    for idx, r in case_df.iterrows():\n",
    "        if r['meta:npSchema:regionName'] in regions:\n",
    "            case_df.loc[idx, 'region'] = regions[r['meta:npSchema:regionName']]\n",
    "        else:\n",
    "            case_df.loc[idx, 'region'] = \"\"\n",
    "            \n",
    "    # Filter to regions available.\n",
    "    case_df = case_df[case_df['region'] != \"\"]\n",
    "    \n",
    "    # Check that all regions are available.\n",
    "    if len(case_df['region'].unique()) != n_regions:\n",
    "        continue\n",
    "    \n",
    "    # For each unique region only take one image.\n",
    "    for region in case_df['region'].unique():\n",
    "        case_df_region = case_df[case_df['region'] == region]\n",
    "        \n",
    "        item = reconstruct_nested(case_df_region.iloc[0].to_dict(), \":\")\n",
    "        item[\"meta\"][\"npSchema\"][\"region\"] = item[\"region\"]\n",
    "        \n",
    "        # Only add some keys.\n",
    "        item = {\n",
    "            \"_id\": item['_id'],\n",
    "            \"name\": item['name'],\n",
    "            \"meta\": item['meta'],\n",
    "        }\n",
    "\n",
    "        # Take the first image.\n",
    "        neurotk_items.append(\n",
    "            item\n",
    "        )\n",
    "        \n",
    "        if case_count < 2:\n",
    "            neurotk_items_small.append(\n",
    "                item\n",
    "            )\n",
    "        \n",
    "    case_count += 1\n",
    "\n",
    "print(\"Total number of items to push to NeuroTK:\", len(neurotk_items))  \n",
    "print(\"For the small dataset:\", len(neurotk_items_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset items.\n",
    "_ = gc.createItem(\n",
    "    \"673512de900c0c0559bf12f0\", \"Verified Tau 4 Regions\", reuseExisting=True,\n",
    "    metadata={\"dataset\": neurotk_items, \"filters\": {}}\n",
    ")\n",
    "_ = gc.createItem(\n",
    "    \"673512de900c0c0559bf12f0\", \"Verified Tau 4 Regions (small)\", \n",
    "    reuseExisting=True, \n",
    "    metadata={\"dataset\": neurotk_items_small, \"filters\": {}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Done!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurotk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
